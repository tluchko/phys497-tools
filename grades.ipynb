{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "1. Download the complete grades from Canvas\n",
    "1. Download the `Individual Student Reports` -> `Score Reports` from the MFT admin portal.  \n",
    "   This is a PDF and is the only way to get percentile scores for the current cohort.\n",
    "1. Manually enter the MFT percentile scores in the `ETS Major Field Test` column.\n",
    "1. Modify the filenames and parameters as needed\n",
    "1. Run the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.interpolate as interp\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import operator\n",
    "\n",
    "pd.options.display.width = 200\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants and adjustable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas_input = '2025-05-22T1418_Grades-PHYS_497-17796-SP2025.csv'\n",
    "canvas_output = 'final_grades.csv'\n",
    "solar_output = 'solar_grades.csv'\n",
    "# grade category weights\n",
    "weights = collections.OrderedDict((\n",
    "    ('Homework and Participation Grade Point', 0.15),\n",
    "    ('Final Presentation', 0.425),\n",
    "    ('Final Report', 0.425))\n",
    ")\n",
    "\n",
    "# boost or penalty thresholds for MFT\n",
    "mft_thresholds = collections.OrderedDict((\n",
    "    ('boost', 30),\n",
    "    ('penalty', 0)\n",
    "))\n",
    "\n",
    "# rounding thresholds for final letter grades\n",
    "final_thresholds = collections.OrderedDict((\n",
    "    (3.7, 4.0),\n",
    "    (3.3, 3.7),\n",
    "    (3.0, 3.3),\n",
    "    (2.7, 3.0),\n",
    "    (2.3, 2.7),\n",
    "    (2.0, 2.3),\n",
    "    (1.7, 2.0),\n",
    "    (1.3, 1.7),\n",
    "    (1.0, 1.3),\n",
    "    (0.7, 1.0),\n",
    "    (0.0, 0)\n",
    "))\n",
    "\n",
    "#“incomplete assignment” 4-point grade thresholds for each category\n",
    "professionalism_thresholds = collections.OrderedDict((\n",
    "    ('Preparation', collections.OrderedDict(((4, 1), (3, 2), (2, 3), (1, 4), (0, 5)))),\n",
    "    ('Participation', collections.OrderedDict(((4, 2), (3, 4), (2, 6), (1, 8), (0, 10)))),\n",
    "    ('Practice', collections.OrderedDict(((4, 1), (3, 1), (2, 2), (1, 3), (0, 4)))),\n",
    "    ('Peer-Review', collections.OrderedDict(((4, 1), (3, 2), (2, 3), (1, 4), (0, 5))))\n",
    "))\n",
    "\n",
    "# Map letter grades (including plus/minus) to numeric 4-point scale\n",
    "numeric_map = collections.OrderedDict((    \n",
    "    ('A', 4.0),  ('A-', 3.7), ('B+', 3.3), ('B', 3.0),  ('B-', 2.7),\n",
    "    ('C+', 2.3), ('C', 2.0),  ('C-', 1.7), ('D+', 1.3), ('D', 1.0),\n",
    "    ('D-', 0.7), ('F', 0.0)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_columns(df, names):\n",
    "    \"\"\"\n",
    "    Finds and returns the column names in a DataFrame that match the given names followed by (\\d+).\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to search for columns.\n",
    "    names (list of str): A list of names to match against the column names in the DataFrame. \n",
    "                         Each name is expected to match a column name followed by ' (\\d+)'.\n",
    "\n",
    "    Returns:\n",
    "    list of str: A list of column names that match the given names.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If no columns are found for a given name or if multiple columns match a given name.\n",
    "\n",
    "    Example:\n",
    "    >>> find_columns(canvas, ['Select five CSUNposium talks to attend'])\n",
    "    ['Select five CSUNposium talks to attend (2211230)']\n",
    "    \"\"\"\n",
    "    columns = []\n",
    "    for name in names:\n",
    "        match_columns = [col for col in canvas.columns if re.match(name + r' \\(\\d+\\)', col)]\n",
    "        if not match_columns:\n",
    "            raise ValueError(f\"No columns found for: {name}\")\n",
    "        if len(match_columns) > 1:\n",
    "            raise ValueError(f\"Multiple columns found for: {name}\")\n",
    "        columns.append(match_columns[0])\n",
    "    return columns\n",
    "\n",
    "def count_complete_assignments(df, columns):\n",
    "    \"\"\"\n",
    "    Counts the number of complete assignments for each row in the specified columns of a DataFrame.\n",
    "\n",
    "    The function assumes that a value > 0 in a column indicates a complete assignment.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the assignment data.\n",
    "    columns (list of str): A list of column names to evaluate for completeness.\n",
    "\n",
    "    Returns:\n",
    "    pandas.Series: A Series containing the count of complete assignments for each row.\n",
    "\n",
    "    Example:\n",
    "    >>> count_complete_assignments(canvas, ['Week 1 Participation (2211279)', 'Week 2 Participation (2211287)'])\n",
    "    0    0\n",
    "    1    2\n",
    "    2    1\n",
    "    dtype: int64\n",
    "    \"\"\"\n",
    "    df = df[columns].copy()\n",
    "    # Convert all values to numeric, forcing errors to NaN\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    # Replace all values in the columns with 1 if the value is > 1\n",
    "    df = df[columns].applymap(lambda x: 1 if x > 1 else x)\n",
    "    # For each row, count the number of values that are non-null and non-zero in the columns\n",
    "    return df[columns].sum(axis=1)\n",
    "\n",
    "def calculate_professionalism_grade(df, columns, thresholds=professionalism_thresholds):\n",
    "    \"\"\"\n",
    "    Calculates the professionalism grade for each student based on the given columns and thresholds.\n",
    "\n",
    "    This function computes a numeric grade for each student by:\n",
    "    1. Calculating the number of missing assignments for each category.\n",
    "    2. Interpolating the numeric grade based on the thresholds for each category.\n",
    "    3. Computing the final numeric grade using an anchored grading system.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the assignment data.\n",
    "    columns (list of str): A list of column names to evaluate for professionalism grading.\n",
    "    thresholds (OrderedDict): A dictionary mapping categories to their respective thresholds for grading.\n",
    "\n",
    "    Returns:\n",
    "    pandas.Series: A Series containing the professionalism grade for each student.\n",
    "\n",
    "    Example:\n",
    "    >>> calculate_professionalism_grade(canvas, ['Preparation', 'Participation', 'Practice', 'Peer-Review'])\n",
    "    2    3.7\n",
    "    3    4.0\n",
    "    4    3.3\n",
    "    dtype: float64\n",
    "    \"\"\"\n",
    "    def compute_final(grades: pd.Series) -> float:\n",
    "        \"\"\"\n",
    "        Compute the anchored final numeric grade from a Series of four category scores:\n",
    "        - Anchor = minimum of the four scores.\n",
    "        - count_next = number of categories >= (anchor + 1.0).\n",
    "        - Raw score:\n",
    "            >=2 at next level → anchor + 0.7\n",
    "            =1 at next level  → anchor + 0.3\n",
    "            else             → anchor\n",
    "        - Snap down to nearest allowed step in the 0.0, 0.3, 0.7, 1.0, …, 4.0 scale.\n",
    "        \"\"\"\n",
    "        anchor = grades.min()\n",
    "        count_next = (grades >= anchor + 1.0).sum()\n",
    "        if count_next >= 2:\n",
    "            raw = anchor + 0.7\n",
    "        elif count_next == 1:\n",
    "            raw = anchor + 0.3\n",
    "        else:\n",
    "            raw = anchor\n",
    "\n",
    "        allowed = sorted(numeric_map.values())\n",
    "        return max(v for v in allowed if v <= raw + 1e-8)\n",
    "    \n",
    "    df = df[columns].copy()\n",
    "    \n",
    "    # Calculate the number of missing assignments\n",
    "    df.loc[1:, :] = df.loc[1, :] - df.loc[1:, :]\n",
    "    \n",
    "    # Assign a numeric grade by interpolating the thresholds\n",
    "    for col in df.columns:\n",
    "        category = re.sub(r' \\(\\d+\\)', '', col)\n",
    "        # Get the threshold for the current column\n",
    "        threshold = thresholds[category]\n",
    "        # Interpolate the numeric grade based on the number of missing assignments\n",
    "        lin_interp = interp.interp1d(list(threshold.values()), list(threshold.keys()), fill_value=(4, 0), bounds_error=False)\n",
    "        df[col] = lin_interp(df[col])\n",
    "    \n",
    "    return df.apply(compute_final, axis=1)\n",
    "\n",
    "def apply_grade_threshold(x, thresholds=final_thresholds):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    for v in thresholds.keys():\n",
    "        if x >= v:\n",
    "            return thresholds[v]\n",
    "    return thresholds.values()[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the canvas spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas = pd.read_csv(canvas_input)\n",
    "# Drop the last row\n",
    "canvas = canvas.drop(canvas.index[-1])\n",
    "canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the MFT results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFT percentile scores should be entered directly into the canvas spreadsheet.  \n",
    "# The custom report spreadsheets do not contain percentile scores.\n",
    "\n",
    "# mft = pd.read_excel('ets/ETSMajorFieldTestsDYOAR_May-21-2025_11-36-13.xlsx')\n",
    "# mft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Professionalism score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preparation assignments\n",
    "prep_assignments = [\n",
    "    \"Anatomy of a research paper\",\n",
    "    \"Next Action\", \n",
    "    \"Schedule your recurring meeting with your advisor\", \n",
    "    \"Next Action Followup\",\n",
    "    \"Read the Whitesides paper\", \n",
    "    \"Read the Schön Report\", \n",
    "    \"Essay for next year's students\",\n",
    "    \"Select five CSUNposium talks to attend\"\n",
    "]\n",
    "# For each assignment, get the column names from the canvas dataframe that is the assignment name plus ' (\\d+)'\n",
    "prep_assignments_columns = find_columns(canvas, prep_assignments)\n",
    "\n",
    "canvas[find_columns(canvas, ['Preparation'])[0]] = count_complete_assignments(canvas, prep_assignments_columns)\n",
    "print(canvas[find_columns(canvas, ['Preparation'])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Peer-Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peer_review_assignments = [\n",
    "    \"Project Outline #1 - peer review\", \n",
    "    \"Practice Presentation #1 - peer review\", \n",
    "    \"Project Outline #2 - peer review\", \n",
    "    \"Project Outline #3 Peer Review\", \n",
    "    \"CSUNposium Reviews\", \n",
    "    \"Final Report Draft #1 - peer reviews\", \n",
    "    \"Practice Presentation #2 - peer reviews\", \n",
    "    \"Final Report Draft #2 - peer reviews\"\n",
    "]\n",
    "peer_review_assignments_columns = find_columns(canvas, peer_review_assignments)\n",
    "canvas[find_columns(canvas, ['Peer-Review'])[0]] = count_complete_assignments(canvas, peer_review_assignments_columns)\n",
    "print(canvas[find_columns(canvas, ['Peer-Review'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Participation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participation_assignments = [\n",
    "    \"Week 1 Participation\",\n",
    "    \"Week 2 Participation\",\n",
    "    \"Week 3 Participation\",\n",
    "    \"Week 4 Participation\",\n",
    "    \"Week 5 Participation\",\n",
    "    \"Week 6 Participation\",\n",
    "    \"Week 7 Participation\",\n",
    "    \"Week 8 Participation\",\n",
    "    # \"Week 9 Participation\",\n",
    "    \"Week 10 Participation\",\n",
    "    \"Week 11 Participation\",\n",
    "    \"Week 12 Participation\",\n",
    "    \"Week 13 Participation\",\n",
    "    \"Week 14 Participation\",\n",
    "    \"Week 15 Participation\",\n",
    "    \"Week 16 Participation\"\n",
    "]\n",
    "participation_assignments_columns = find_columns(canvas, participation_assignments)\n",
    "canvas[find_columns(canvas, ['Participation'])[0]] = count_complete_assignments(canvas, participation_assignments_columns)\n",
    "print(canvas[find_columns(canvas, ['Participation'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Practice and Drafts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice_assignments = [\n",
    "    \"Create your elevator pitch\", \n",
    "    \"Project Outline #1\", \n",
    "    \"Practice Presentation #1\",\n",
    "    \"Project Outline #2\",\n",
    "    \"Project Outline #3\",\n",
    "    \"Final Report Draft #1\",\n",
    "    \"Practice Presentation #2\", \n",
    "    \"Final Report Draft #2\"\n",
    "]\n",
    "practice_assignments_columns = find_columns(canvas, practice_assignments)\n",
    "canvas[find_columns(canvas, ['Practice'])[0]] = count_complete_assignments(canvas, practice_assignments_columns)\n",
    "print(canvas[find_columns(canvas, ['Practice'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Professionalism grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas[find_columns(canvas, ['Homework and Participation Grade Point'])[0]] = (\n",
    "    calculate_professionalism_grade(canvas, find_columns(canvas, ['Preparation', 'Participation', 'Practice', 'Peer-Review'])))\n",
    "print(canvas[find_columns(canvas, ['Homework and Participation Grade Point'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the final grade without the MFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas = canvas.set_index('Student')\n",
    "# print(canvas[find_columns(canvas, weights.keys())])\n",
    "final_col = find_columns(canvas, ['Final'])[0]\n",
    "\n",
    "# compute the dot product of the columns of the Professionalism, Final Presentation, and Final Report columns with the weights\n",
    "canvas[final_col] = (\n",
    "    canvas[find_columns(canvas, weights.keys())].apply(pd.to_numeric, errors='coerce') @ list(weights.values()))\n",
    "\n",
    "print(canvas[[final_col]])\n",
    "# round to a letter-grade numeric-value using the thresholds\n",
    "canvas[final_col] = canvas[final_col].apply(lambda x: apply_grade_threshold(x, final_thresholds))\n",
    "\n",
    "print(canvas[[final_col]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the MFT correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_grade_by_one(x, operator):\n",
    "    \"\"\"\n",
    "    Adjusts the grade by one level based on the given modifier.\n",
    "\n",
    "    Parameters:\n",
    "    x (float): The original grade.\n",
    "    mod (int): The modifier to adjust the grade.\n",
    "\n",
    "    Returns:\n",
    "    float: The adjusted grade.\n",
    "    \"\"\"\n",
    "    values = list(sorted(numeric_map.values()))\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    \n",
    "    return values[max(min(operator(values.index(x),1), len(values) - 1), 0)]\n",
    "\n",
    "# apply the MFT modifier to the MFT score\n",
    "final_col = find_columns(canvas, ['Final'])[0]\n",
    "mft_col = find_columns(canvas, ['ETS Major Field Test'])[0]\n",
    "# canvas[final_col] = canvas[final_col].apply(lambda x: mod_grade_by_one(x, operator.sub))\n",
    "\n",
    "boost_mask = canvas[mft_col] > mft_thresholds['boost']\n",
    "canvas.loc[boost_mask,final_col] = canvas.loc[boost_mask, final_col].apply(lambda x: mod_grade_by_one(x, operator.add))\n",
    "drop_mask = canvas[mft_col] < mft_thresholds['penalty']\n",
    "canvas.loc[drop_mask,final_col] = canvas.loc[drop_mask, final_col].apply(lambda x: mod_grade_by_one(x, operator.sub))\n",
    "\n",
    "print(canvas[final_col])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output the final grades for Canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canvas.reset_index().to_csv(canvas_output, index=False)\n",
    "canvas.reset_index()[['Student', 'ID', 'SIS User ID', 'SIS Login ID', 'Section'] \n",
    "                     + find_columns(canvas, ['Preparation', 'Practice', 'Participation', 'Peer-Review', 'Homework and Participation Grade Point', 'ETS Major Field Test', 'Final'])].to_csv(canvas_output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output the final grades for SOLAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas.loc[:,'Final Letter Grade'] = canvas[final_col].apply(lambda x: next((k for k, v in numeric_map.items() if v == x), None))\n",
    "# print(canvas[['Final Letter Grade']])\n",
    "canvas.reset_index()[['SIS User ID','Final Letter Grade']].to_csv(solar_output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
